{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc552e94-fb6a-4365-a7f9-2ae024a6fda5",
   "metadata": {},
   "source": [
    "## NLP basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0a9a5-fc42-40fb-8905-2e6ffee63aae",
   "metadata": {},
   "source": [
    "There are different ways to preprocess text: \n",
    "\n",
    " * **stop word removal**, \n",
    "\n",
    "* **tokenization**, \n",
    "\n",
    "* **stemming**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18071f5c-72a7-4f65-a9b7-873f5aab3ffe",
   "metadata": {},
   "source": [
    "### Tokenization. \n",
    "\n",
    "    Itâ€™s the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87344243-cb8e-4e18-97cb-333e39ebb1ee",
   "metadata": {},
   "source": [
    "#### NLTK Word Tokenize\n",
    "    NLTK (Natural Language Toolkit)NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57cec11d-4177-4daf-b5d7-39ff937b5793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize,sent_tokenize,wordpunct_tokenize,TreebankWordTokenizer,TweetTokenizer,MWETokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64b41608-4ea2-4e25-a9ef-df90d28a8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The Sole meaning of life, is to serve humanity, #we.are.humans.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80902347-ad14-42c7-b640-fccabc4e28e6",
   "metadata": {},
   "source": [
    "#### 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28a6a1e7-2d3b-4efe-b1a8-e18c2be07e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenize: ['The', 'Sole', 'meaning', 'of', 'life', ',', 'is', 'to', 'serve', 'humanity', ',', '#', 'we.are.humans', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Word Tokenize: {word_tokenize(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15dcf082-6ff4-4ec5-9dde-feec98803cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Sole',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'life',\n",
       " ',',\n",
       " 'is',\n",
       " 'to',\n",
       " 'serve',\n",
       " 'humanity',\n",
       " ',',\n",
       " '#',\n",
       " 'we.are.humans',\n",
       " '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "576af92d-a0b6-412e-bf8a-1f71cfae1c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Sole',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'life,',\n",
       " 'is',\n",
       " 'to',\n",
       " 'serve',\n",
       " 'humanity,',\n",
       " '#we.are.humans.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using split \n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f366f-850e-4cdf-83d2-133505ee2734",
   "metadata": {},
   "source": [
    "It included the punctuation mark: \".\" & \",\" so to avoid that nltk is used for better & fast processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32c73fc1-5107-4474-9b5c-8a03c4d74d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Sole meaning of life', 'is to serve humanity', '#we.are.humans.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using split \n",
    "text.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ab30b-af34-4ed7-aad6-272edf2cac79",
   "metadata": {},
   "source": [
    "#### 2. Sentence Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f432ceec-3720-47b2-8f5b-f29ada974350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenize: ['The Sole meaning of life, is to serve humanity, #we.are.humans.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Sentence Tokenize: {sent_tokenize(text)}')\n",
    "#N.B: The sent_tokenize uses the pre-trained model from tokenizers/punkt/english.pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a75151-4962-4877-844c-f44fc27a07d9",
   "metadata": {},
   "source": [
    "#### 3. Punctuation-based tokenizer:\n",
    "    This tokenizer splits the sentences into words based on whitespaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a84928e2-3935-46ff-b369-3b67932725dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordpunct_tokenize: ['The', 'Sole', 'meaning', 'of', 'life', ',', 'is', 'to', 'serve', 'humanity', ',', '#', 'we', '.', 'are', '.', 'humans', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'wordpunct_tokenize: {wordpunct_tokenize(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416e33f-0cd4-4685-a5cd-36775310165b",
   "metadata": {},
   "source": [
    "We could notice the difference between considering â€œwe.are.humansâ€ a word in word_tokenize and split it in the wordpunct_tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589eab04-90e7-4c74-a7ad-eecc83b3cca0",
   "metadata": {},
   "source": [
    "#### 4. Treebank Word tokenizer\n",
    "    It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token. Besides, it contains rules for English contractions. \n",
    "\n",
    "    For example â€œdonâ€™tâ€ is tokenized as [â€œdoâ€, â€œnâ€™tâ€]. You can find all the rules for the Treebank Tokenizer at this link. \n",
    "\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5caf7d01-f711-4726-bd24-8f97dcc70968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Word tokenizer: ['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others']\n"
     ]
    }
   ],
   "source": [
    "text = \"What you don't want to be done to yourself, don't do to others\"\n",
    "tokenizer = TreebankWordTokenizer() # takes no arguments, so we create an object\n",
    "print(f'Treebank Word tokenizer: {tokenizer.tokenize(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "756971a8-cd93-442e-84a4-b64d39f5f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordpunct_tokenize: ['What', 'you', 'don', \"'\", 't', 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'don', \"'\", 't', 'do', 'to', 'others']\n"
     ]
    }
   ],
   "source": [
    "print(f'wordpunct_tokenize: {wordpunct_tokenize(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a80162-fbb7-4ea6-a05c-531ad461bcee",
   "metadata": {},
   "source": [
    "#### 5. Tweet tokenizer\n",
    "    When we want to apply tokenization in text data like tweets, the tokenizers mentioned above canâ€™t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb985dfb-5377-4e7c-b174-9e554c21ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet tokenizer: [\"Don't\", 'take', 'crytocurrency', 'advice', 'from', 'people', 'on', 'twitter', 'ðŸ˜Š', 'ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"Don't take crytocurrency advice from people on twitter ðŸ˜ŠðŸ˜‚\"\n",
    "tokenizer = TweetTokenizer() # takes no arguments, so we create an object\n",
    "print(f'Tweet tokenizer: {tokenizer.tokenize(tweet)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8659f-a43d-42ea-9fd7-5e2d9954c13c",
   "metadata": {},
   "source": [
    "#### 6. MWET tokenizer\n",
    "    NLTKâ€™s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f6f5bf8-3397-4dc0-8a21-9db46cbfb11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWET: ['H', 'o', 'p', 'e', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'i', 'n', 'g', ' ', 's', 't', 'r', 'o', 'n', 'g', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'f', 'e', 'a', 'r', '!', ' ', 'H', 'u', 'n', 'g', 'e', 'r', ' ', 'g', 'a', 'm', 'e', 's', ' ', '#', 'H', 'O', 'P', 'E']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hope is the only thing stronger than fear! Hunger games #HOPE\"\n",
    "tokenizer = MWETokenizer()\n",
    "print(f'MWET: {tokenizer.tokenize(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a984609-0fff-4575-8a6f-e18803be63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWET: ['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'games', '#', 'HOPE']\n"
     ]
    }
   ],
   "source": [
    "print(f'MWET: {tokenizer.tokenize(word_tokenize(text))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8eaac58c-4c6c-43de-9a9b-5b1141c75533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWET: ['Hope', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_games', '#', 'HOPE']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_mwe(('Hunger','games'))\n",
    "print(f'MWET: {tokenizer.tokenize(word_tokenize(text))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d781eeb-d48d-436e-adaf-f30a3eb39b0f",
   "metadata": {},
   "source": [
    "### TextBlob Word Tokenize\n",
    "    TextBlob is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "Letâ€™s start by installing TextBlob and the NLTK corpora:\n",
    "\n",
    "    pip install -U textblob \n",
    "    python3 -m textblob.download_corpora\n",
    "    \n",
    "    In the code below, we perform word tokenization using TextBlob library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "411b53ef-f50a-4d7d-a234-7dbfd7f09c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85864a1a-b69d-49f7-a38e-274b8abe5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6f039c0-3723-44ed-b679-c2ea563be1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text = \"But I am glad that you'll see me as I am. Above all, I wouldn't want people to think that I want to prove anything but I don't want.\"\\\n",
    "\n",
    "blob_object = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8abf2bf-5ee1-445c-b4e6-a81e08683d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['But', 'I', 'am', 'glad', 'that', 'you', \"'ll\", 'see', 'me', 'as', 'I', 'am', 'Above', 'all', 'I', 'would', \"n't\", 'want', 'people', 'to', 'think', 'that', 'I', 'want', 'to', 'prove', 'anything', 'but', 'I', 'do', \"n't\", 'want']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization of the text\n",
    "text_words = blob_object.words\n",
    "\n",
    "# To see all tokens\n",
    "print(text_words)\n",
    "# To count no. of tokens\n",
    "print(len(text_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032963b-64b5-4f0a-affe-319e2c3e026e",
   "metadata": {},
   "source": [
    "We could notice that the TextBlob tokenizer removes the punctuations. In addition, it has rules for English contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e5cd7-7637-4031-9cb3-340c92354a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e09c6-f18b-4722-834b-9956b83bbad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
